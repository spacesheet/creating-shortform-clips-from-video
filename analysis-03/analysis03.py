# -*- coding: utf-8 -*-

import os
import json
import subprocess
from datetime import timedelta
from faster_whisper import WhisperModel
from pathlib import Path
import torch
import numpy as np
from sentence_transformers import SentenceTransformer
from collections import Counter
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import warnings
import shutil
warnings.filterwarnings('ignore')

# í˜•íƒœì†Œ ë¶„ì„
try:
    from kiwipiepy import Kiwi
    USE_KIWI = True
except ImportError:
    try:
        from konlpy.tag import Okt
        USE_KIWI = False
    except ImportError:
        USE_KIWI = None


class ShortsAnalyzer:
    """ì‡¼ì¸  ì˜ìƒ ë¶„ì„ ë° í•„í„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self, community_keywords=None):
        self.community_keywords = community_keywords or []
        
        if USE_KIWI:
            print("ğŸ”§ Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë”©...")
            self.kiwi = Kiwi()
        elif USE_KIWI is False:
            print("ğŸ”§ Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë”©...")
            self.okt = Okt()
        
        self.positive_words = set([
            'ì¢‹ë‹¤', 'ìµœê³ ', 'ëŒ€ë°•', 'ì¬ë°Œë‹¤', 'ë©‹ì§€ë‹¤', 'í›Œë¥­í•˜ë‹¤', 'ì™„ë²½', 'ê°ë™',
            'í–‰ë³µ', 'ì‚¬ë‘', 'ì›ƒë‹¤', 'ê¸°ì˜ë‹¤', 'ì¦ê²ë‹¤', 'ìœ ì¾Œ', 'í¥ë¯¸ë¡­ë‹¤',
            'ë†€ëë‹¤', 'ì‹ ê¸°í•˜ë‹¤', 'í™˜ìƒì ', 'êµ‰ì¥í•˜ë‹¤', 'íƒì›”í•˜ë‹¤', 'ì˜ˆì˜ë‹¤',
            'ì•„ë¦„ë‹µë‹¤', 'ê·€ì—½ë‹¤', 'ë‹¬ì½¤í•˜ë‹¤', 'ë§›ìˆë‹¤', 'ì‹ ë‚˜ë‹¤', 'í™”ë ¤í•˜ë‹¤'
        ])
        
        self.negative_words = set([
            'ë‚˜ì˜ë‹¤', 'ìµœì•…', 'ì§œì¦', 'ì§€ë£¨í•˜ë‹¤', 'ì‹¤ë§', 'ë³„ë¡œ', 'í›„íšŒ',
            'ìŠ¬í”„ë‹¤', 'ìš°ìš¸', 'í™”ë‚˜ë‹¤', 'ë¯¸ì¹˜ë‹¤', 'í˜ë“¤ë‹¤', 'ì•„í”„ë‹¤', 'ë¶ˆí¸',
            'ì‹«ë‹¤', 'ë¬´ì„­ë‹¤', 'ê±±ì •', 'ë¬¸ì œ', 'ì‹¤íŒ¨', 'ë”ì°í•˜ë‹¤', 'ë”ëŸ½ë‹¤'
        ])
        
        print(f"âœ… ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ (ì»¤ë®¤ë‹ˆí‹° í‚¤ì›Œë“œ: {len(self.community_keywords)}ê°œ)")
    
    def extract_morphemes(self, text):
        """í˜•íƒœì†Œ ë¶„ì„ ë° ëª…ì‚¬/ë™ì‚¬ ì¶”ì¶œ"""
        if USE_KIWI:
            result = self.kiwi.analyze(text)
            morphemes = []
            for token in result[0][0]:
                if token.tag in ['NNG', 'NNP', 'VV', 'VA']:
                    morphemes.append(token.form)
            return morphemes
        elif USE_KIWI is False:
            return self.okt.nouns(text) + [word for word, pos in self.okt.pos(text) if pos in ['Verb', 'Adjective']]
        else:
            return re.findall(r'[\wê°€-í£]+', text)
    
    def clean_words(self, words):
        """ë¶ˆìš©ì–´ ì œê±° ë° ë‹¨ì–´ ì •ì œ"""
        stop_words = set([
            'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ì™€', 'ê³¼', 
            'ë„', 'ìœ¼ë¡œ', 'ë¡œ', 'ì—ì„œ', 'í•˜ë‹¤', 'ìˆë‹¤', 'ë˜ë‹¤', 'ì´ë‹¤',
            'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ì¢€', 'ë§‰', 'ì§„ì§œ', 'ë˜ê²Œ',
            'ê±°', 'ê²Œ', 'ë„¤', 'ìš”', 'ìŒ', 'ì–´', 'ì•„', 'ì•¼', 'ì„', 'ë“¤',
            'ë•Œ', 'ì •ë„', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ë§ˆë‹¤', 'ì¡°ì°¨', 'ë‚˜', 'ì´ëŸ°',
            'ê°™ë‹¤', 'ë³´ë‹¤', 'ìœ„í•˜ë‹¤', 'ëŒ€í•˜ë‹¤', 'í†µí•˜ë‹¤', 'ê´€í•˜ë‹¤'
        ])
        
        cleaned = []
        for word in words:
            if len(word) >= 2 and word not in stop_words:
                cleaned.append(word)
        
        return cleaned
    
    def analyze_words(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ ë° ì •ì œ"""
        morphemes = self.extract_morphemes(text)
        cleaned_words = self.clean_words(morphemes)
        word_freq = Counter(cleaned_words)
        
        return {
            'words': cleaned_words,
            'word_frequency': dict(word_freq.most_common(20)),
            'unique_words': len(word_freq),
            'total_words': len(cleaned_words)
        }
    
    def calculate_community_match(self, short_words):
        """ì»¤ë®¤ë‹ˆí‹° í‚¤ì›Œë“œì™€ì˜ ë§¤ì¹­ë„ ê³„ì‚°"""
        if not self.community_keywords:
            return 0.0, []
        
        community_set = set([kw.lower() for kw in self.community_keywords])
        short_set = set([w.lower() for w in short_words])
        
        matched_keywords = community_set.intersection(short_set)
        
        if len(community_set) == 0:
            match_score = 0.0
        else:
            match_score = len(matched_keywords) / len(community_set)
        
        return match_score, list(matched_keywords)
    
    def sentiment_analysis(self, text, words):
        """ê°ì • ë¶„ì„ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½)"""
        positive_count = sum(1 for word in words if word in self.positive_words)
        negative_count = sum(1 for word in words if word in self.negative_words)
        
        total_sentiment_words = positive_count + negative_count
        
        if total_sentiment_words == 0:
            return 'neutral', 0.0
        
        positive_ratio = positive_count / total_sentiment_words
        
        if positive_ratio > 0.6:
            sentiment = 'positive'
        elif positive_ratio < 0.4:
            sentiment = 'negative'
        else:
            sentiment = 'neutral'
        
        sentiment_score = (positive_count - negative_count) / max(len(words), 1)
        
        return sentiment, sentiment_score
    
    def topic_modeling(self, texts, n_topics=5):
        """ì£¼ì œ ëª¨ë¸ë§ (LDA)"""
        if len(texts) < n_topics:
            n_topics = max(1, len(texts))
        
        vectorizer = TfidfVectorizer(
            max_features=100,
            min_df=1,
            max_df=0.8
        )
        
        try:
            tfidf_matrix = vectorizer.fit_transform(texts)
            
            lda = LatentDirichletAllocation(
                n_components=n_topics,
                random_state=42,
                max_iter=20
            )
            lda.fit(tfidf_matrix)
            
            feature_names = vectorizer.get_feature_names_out()
            topics = []
            
            for topic_idx, topic in enumerate(lda.components_):
                top_words_idx = topic.argsort()[-5:][::-1]
                top_words = [feature_names[i] for i in top_words_idx]
                topics.append({
                    'topic_id': topic_idx,
                    'keywords': top_words
                })
            
            doc_topics = lda.transform(tfidf_matrix)
            doc_topic_assignments = doc_topics.argmax(axis=1)
            
            return topics, doc_topic_assignments
        
        except Exception as e:
            print(f"âš ï¸  ì£¼ì œ ëª¨ë¸ë§ ì‹¤íŒ¨: {e}")
            return [], [0] * len(texts)
    
    def generate_title(self, text, keywords, sentiment, topic_words):
        """ì œëª© ìë™ ìƒì„±"""
        main_keywords = keywords[:3] if keywords else []
        
        sentiment_prefix = {
            'positive': 'ğŸ’¡',
            'negative': 'âš ï¸',
            'neutral': 'ğŸ“Œ'
        }
        prefix = sentiment_prefix.get(sentiment, 'ğŸ“Œ')
        
        if topic_words:
            title_base = ' '.join(topic_words[:2])
        elif main_keywords:
            title_base = ' '.join(main_keywords)
        else:
            words = text.split()[:10]
            title_base = ' '.join(words)
        
        if len(title_base) > 30:
            title_base = title_base[:30] + '...'
        
        title = f"{prefix} {title_base}"
        
        return title


class ShortsBatchProcessor:
    """ì‡¼ì¸  í´ë” ë°°ì¹˜ ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, shorts_folder, output_dir="filtered_shorts", 
                 community_keywords=None, match_threshold=0.2):
        """
        Args:
            shorts_folder: ì‡¼ì¸  ì˜ìƒë“¤ì´ ìˆëŠ” í´ë” ê²½ë¡œ
            output_dir: í•„í„°ë§ëœ ì‡¼ì¸ ë¥¼ ì €ì¥í•  í´ë”
            community_keywords: ì»¤ë®¤ë‹ˆí‹° í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            match_threshold: í•„í„°ë§ ì„ê³„ê°’
        """
        self.shorts_folder = shorts_folder
        self.output_dir = output_dir
        self.match_threshold = match_threshold
        os.makedirs(output_dir, exist_ok=True)
        
        # ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.analyzer = ShortsAnalyzer(community_keywords=community_keywords)
        
        # Whisper ëª¨ë¸ ë¡œë“œ
        print("ğŸ¤– Whisper ìŒì„± ì¸ì‹ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.whisper_model = WhisperModel(
            "base", 
            device="cpu", 
            compute_type="int8"
        )
        
        print(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ì…ë ¥ í´ë”: {shorts_folder}")
        print(f"   ì¶œë ¥ í´ë”: {output_dir}")
    
    def get_video_files(self):
        """í´ë”ì—ì„œ ë¹„ë””ì˜¤ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        video_extensions = ['.mp4', '.mov', '.avi', '.mkv', '.flv', '.wmv', '.webm']
        video_files = []
        
        for filename in os.listdir(self.shorts_folder):
            file_path = os.path.join(self.shorts_folder, filename)
            if os.path.isfile(file_path):
                ext = os.path.splitext(filename)[1].lower()
                if ext in video_extensions:
                    video_files.append(file_path)
        
        return sorted(video_files)
    
    def get_video_duration(self, video_path):
        """ë¹„ë””ì˜¤ ê¸¸ì´ ê°€ì ¸ì˜¤ê¸°"""
        cmd = [
            'ffprobe',
            '-v', 'quiet',
            '-print_format', 'json',
            '-show_format',
            video_path
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            info = json.loads(result.stdout)
            return float(info['format']['duration'])
        except:
            return 0.0
    
    def transcribe_video(self, video_path):
        """ë¹„ë””ì˜¤ ìŒì„± ì¸ì‹"""
        try:
            segments_generator, info = self.whisper_model.transcribe(
                video_path,
                language="ko",
                word_timestamps=True,
                vad_filter=True,
                beam_size=5
            )
            
            segments = []
            for segment in segments_generator:
                segments.append({
                    'start': segment.start,
                    'end': segment.end,
                    'text': segment.text
                })
            
            full_text = ' '.join([seg['text'] for seg in segments])
            
            return {
                'segments': segments,
                'text': full_text,
                'language': info.language
            }
        
        except Exception as e:
            print(f"      âš ï¸  ìŒì„± ì¸ì‹ ì‹¤íŒ¨: {e}")
            return None
    
    def analyze_short(self, video_path):
        """ë‹¨ì¼ ì‡¼ì¸  ë¶„ì„"""
        filename = os.path.basename(video_path)
        
        print(f"\n   ğŸ“¹ {filename}")
        
        # 1. ìŒì„± ì¸ì‹
        print(f"      ğŸ¤ ìŒì„± ì¸ì‹ ì¤‘...")
        transcription = self.transcribe_video(video_path)
        
        if not transcription or not transcription['text'].strip():
            print(f"      âŒ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        text = transcription['text']
        print(f"      ğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({len(text)}ì)")
        
        # 2. ë‹¨ì–´ ë¶„ì„
        word_analysis = self.analyzer.analyze_words(text)
        words = word_analysis['words']
        
        # 3. ì»¤ë®¤ë‹ˆí‹° ë§¤ì¹­
        match_score, matched_kws = self.analyzer.calculate_community_match(words)
        
        # 4. ê°ì • ë¶„ì„
        sentiment, sentiment_score = self.analyzer.sentiment_analysis(text, words)
        
        # 5. ë¹„ë””ì˜¤ ì •ë³´
        duration = self.get_video_duration(video_path)
        
        result = {
            'filename': filename,
            'path': video_path,
            'duration': duration,
            'text': text,
            'word_analysis': word_analysis,
            'match_score': match_score,
            'matched_keywords': matched_kws,
            'sentiment': sentiment,
            'sentiment_score': sentiment_score,
            'transcription': transcription
        }
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"      ğŸ¯ ë§¤ì¹­: {match_score:.1%} | ğŸ˜Š ê°ì •: {sentiment} ({sentiment_score:.2f})")
        if matched_kws:
            print(f"      âœ… í‚¤ì›Œë“œ: {', '.join(matched_kws[:5])}")
        
        return result
    
    def process_all_shorts(self):
        """ëª¨ë“  ì‡¼ì¸  ì²˜ë¦¬"""
        video_files = self.get_video_files()
        
        if not video_files:
            print(f"âŒ {self.shorts_folder}ì—ì„œ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return [], []
        
        print(f"\nğŸ“Š ì´ {len(video_files)}ê°œì˜ ì‡¼ì¸  ë°œê²¬")
        print("="*80)
        
        all_results = []
        
        for i, video_path in enumerate(video_files, 1):
            print(f"\n[{i}/{len(video_files)}] ë¶„ì„ ì¤‘...")
            
            result = self.analyze_short(video_path)
            
            if result:
                all_results.append(result)
        
        print("\n" + "="*80)
        print(f"âœ… ë¶„ì„ ì™„ë£Œ: {len(all_results)}ê°œ ì„±ê³µ")
        
        return all_results
    
    def add_topic_modeling(self, results):
        """ì£¼ì œ ëª¨ë¸ë§ ìˆ˜í–‰"""
        if len(results) < 2:
            for result in results:
                result['topic_model_id'] = 0
                result['topic_keywords'] = []
            return results
        
        print(f"\nğŸ§  ì£¼ì œ ëª¨ë¸ë§ ìˆ˜í–‰ ì¤‘...")
        
        texts = [r['text'] for r in results]
        n_topics = min(5, max(2, len(results) // 3))
        topics, doc_topics = self.analyzer.topic_modeling(texts, n_topics=n_topics)
        
        for i, result in enumerate(results):
            topic_id = doc_topics[i] if i < len(doc_topics) else 0
            topic_info = topics[topic_id] if topic_id < len(topics) else {'keywords': []}
            
            result['topic_model_id'] = topic_id
            result['topic_keywords'] = topic_info['keywords']
        
        print(f"âœ… {len(topics)}ê°œ ì£¼ì œ ë°œê²¬")
        for topic in topics:
            print(f"   ì£¼ì œ {topic['topic_id']+1}: {', '.join(topic['keywords'])}")
        
        return results
    
    def generate_titles(self, results):
        """ì œëª© ìƒì„±"""
        print(f"\nâœï¸  ì œëª© ìƒì„± ì¤‘...")
        
        for result in results:
            title = self.analyzer.generate_title(
                text=result['text'],
                keywords=list(result['word_analysis']['word_frequency'].keys()),
                sentiment=result['sentiment'],
                topic_words=result.get('topic_keywords', [])
            )
            result['title'] = title
        
        return results
    
    def filter_and_copy_shorts(self, results):
        """í•„í„°ë§ ë° íŒŒì¼ ë³µì‚¬"""
        print(f"\nğŸ“‚ í•„í„°ë§ ë° íŒŒì¼ ë³µì‚¬ ì¤‘...")
        print(f"   ë§¤ì¹­ ì„ê³„ê°’: {self.match_threshold:.0%}")
        
        filtered_results = []
        
        for i, result in enumerate(results, 1):
            if result['match_score'] >= self.match_threshold:
                # ìƒˆ íŒŒì¼ëª… ìƒì„± (ë§¤ì¹­ ì ìˆ˜ í¬í•¨)
                original_name = os.path.splitext(result['filename'])[0]
                original_ext = os.path.splitext(result['filename'])[1]
                new_filename = f"{i:03d}_{result['match_score']:.0%}_{original_name}{original_ext}"
                new_path = os.path.join(self.output_dir, new_filename)
                
                # íŒŒì¼ ë³µì‚¬
                try:
                    shutil.copy2(result['path'], new_path)
                    result['filtered_filename'] = new_filename
                    result['filtered_path'] = new_path
                    filtered_results.append(result)
                    print(f"   âœ… [{i}] {new_filename}")
                except Exception as e:
                    print(f"   âŒ [{i}] ë³µì‚¬ ì‹¤íŒ¨: {e}")
            else:
                print(f"   â­ï¸  [{i}] {result['filename']} (ë§¤ì¹­ {result['match_score']:.1%} < {self.match_threshold:.0%})")
        
        return filtered_results
    
    def _format_time(self, seconds):
        """ì´ˆë¥¼ HH:MM:SS í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        return str(timedelta(seconds=int(seconds)))
    
    def save_reports(self, all_results, filtered_results):
        """ë¦¬í¬íŠ¸ ì €ì¥"""
        
        # 1. JSON ë¦¬í¬íŠ¸
        json_path = os.path.join(self.output_dir, "analysis_report.json")
        
        report = {
            'shorts_folder': self.shorts_folder,
            'settings': {
                'match_threshold': self.match_threshold,
                'community_keywords': self.analyzer.community_keywords
            },
            'statistics': {
                'total_shorts': len(all_results),
                'filtered_shorts': len(filtered_results),
                'filter_rate': len(filtered_results) / max(len(all_results), 1)
            },
            'filtered_shorts': [
                {
                    'original_filename': r['filename'],
                    'filtered_filename': r.get('filtered_filename'),
                    'title': r.get('title'),
                    'duration': r['duration'],
                    'match_score': r['match_score'],
                    'matched_keywords': r['matched_keywords'],
                    'sentiment': r['sentiment'],
                    'sentiment_score': r['sentiment_score'],
                    'topic_keywords': r.get('topic_keywords', []),
                    'word_frequency': r['word_analysis']['word_frequency'],
                    'text': r['text']
                }
                for r in filtered_results
            ],
            'all_shorts_summary': [
                {
                    'filename': r['filename'],
                    'match_score': r['match_score'],
                    'sentiment': r['sentiment'],
                    'filtered': r in filtered_results
                }
                for r in all_results
            ]
        }
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“Š JSON ë¦¬í¬íŠ¸ ì €ì¥: {json_path}")
        
        # 2. í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸
        txt_path = os.path.join(self.output_dir, "report.txt")
        
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("ì‡¼ì¸  ë°°ì¹˜ ë¶„ì„ ë¦¬í¬íŠ¸\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"ì…ë ¥ í´ë”: {self.shorts_folder}\n")
            f.write(f"ì „ì²´ ì‡¼ì¸ : {len(all_results)}ê°œ\n")
            f.write(f"í•„í„°ë§ í†µê³¼: {len(filtered_results)}ê°œ\n")
            f.write(f"í•„í„°ë§ ë¹„ìœ¨: {len(filtered_results)/max(len(all_results), 1):.1%}\n")
            f.write(f"ì»¤ë®¤ë‹ˆí‹° í‚¤ì›Œë“œ: {', '.join(self.analyzer.community_keywords[:20])}\n")
            f.write(f"ë§¤ì¹­ ì„ê³„ê°’: {self.match_threshold:.0%}\n\n")
            
            f.write("=" * 80 + "\n")
            f.write("í•„í„°ë§ëœ ì‡¼ì¸  ìƒì„¸ ì •ë³´\n")
            f.write("=" * 80 + "\n")
            
            for i, result in enumerate(filtered_results, 1):
                f.write(f"\n{'='*80}\n")
                f.write(f"[{i}] {result.get('title', 'N/A')}\n")
                f.write(f"{'='*80}\n")
                f.write(f"ì›ë³¸ íŒŒì¼: {result['filename']}\n")
                f.write(f"ìƒˆ íŒŒì¼ëª…: {result.get('filtered_filename')}\n")
                f.write(f"ê¸¸ì´: {self._format_time(result['duration'])} ({result['duration']:.1f}ì´ˆ)\n")
                f.write(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼:\n")
                f.write(f"  - ì»¤ë®¤ë‹ˆí‹° ë§¤ì¹­: {result['match_score']:.1%}\n")
                f.write(f"  - ë§¤ì¹­ëœ í‚¤ì›Œë“œ: {', '.join(result['matched_keywords']) if result['matched_keywords'] else 'ì—†ìŒ'}\n")
                f.write(f"  - ê°ì •: {result['sentiment']} (ì ìˆ˜: {result['sentiment_score']:.2f})\n")
                f.write(f"  - ì£¼ì œ í‚¤ì›Œë“œ: {', '.join(result.get('topic_keywords', []))}\n")
                f.write(f"\nğŸ”¤ ì£¼ìš” ë‹¨ì–´:\n")
                for word, freq in list(result['word_analysis']['word_frequency'].items())[:10]:
                    f.write(f"  - {word}: {freq}íšŒ\n")
                f.write(f"\nğŸ“ ì „ì²´ í…ìŠ¤íŠ¸:\n")
                f.write(f"  {result['text']}\n")
            
            f.write("\n\n" + "=" * 80 + "\n")
            f.write("ì „ì²´ ì‡¼ì¸  ìš”ì•½ (ë§¤ì¹­ ì ìˆ˜ìˆœ)\n")
            f.write("=" * 80 + "\n\n")
            
            sorted_results = sorted(all_results, key=lambda x: x['match_score'], reverse=True)
            
            f.write(f"{'ìˆœìœ„':<6} {'íŒŒì¼ëª…':<40} {'ë§¤ì¹­':<8} {'ê°ì •':<10} {'í•„í„°ë§'}\n")
            f.write("-" * 80 + "\n")
            
            for i, result in enumerate(sorted_results, 1):
                filtered_mark = "âœ…" if result in filtered_results else "âŒ"
                f.write(f"{i:<6} {result['filename']:<40} {result['match_score']:>6.1%}  "
                       f"{result['sentiment']:<10} {filtered_mark}\n")
        
        print(f"ğŸ“„ í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ì €ì¥: {txt_path}")
        
        # 3. CSV ë¦¬í¬íŠ¸ (ê°„ë‹¨ ë²„ì „)
        csv_path = os.path.join(self.output_dir, "summary.csv")
        
        with open(csv_path, 'w', encoding='utf-8-sig') as f:  # Excel í˜¸í™˜ì„ ìœ„í•´ utf-8-sig
            f.write("ìˆœìœ„,íŒŒì¼ëª…,ë§¤ì¹­ì ìˆ˜,ë§¤ì¹­í‚¤ì›Œë“œ,ê°ì •,ê°ì •ì ìˆ˜,í•„í„°ë§ì—¬ë¶€,ì£¼ìš”ë‹¨ì–´\n")
            
            sorted_results = sorted(all_results, key=lambda x: x['match_score'], reverse=True)
            
            for i, result in enumerate(sorted_results, 1):
                filtered = "í†µê³¼" if result in filtered_results else "ì œì™¸"
                matched_kws = '; '.join(result['matched_keywords'])
                top_words = '; '.join(list(result['word_analysis']['word_frequency'].keys())[:5])
                
                f.write(f"{i},{result['filename']},{result['match_score']:.2%},"
                       f"\"{matched_kws}\",{result['sentiment']},{result['sentiment_score']:.2f},"
                       f"{filtered},\"{top_words}\"\n")
        
        print(f"ğŸ“Š CSV ë¦¬í¬íŠ¸ ì €ì¥: {csv_path}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # Segmentation fault ë°©ì§€
    os.environ["OMP_NUM_THREADS"] = "1"
    os.environ["MKL_NUM_THREADS"] = "1"
    
    try:
        torch.set_num_threads(1)
    except ImportError:
        pass
    
    # ==================== ì„¤ì • ====================
    SHORTS_FOLDER = "fast_shorts"  # ì‡¼ì¸  ì˜ìƒë“¤ì´ ìˆëŠ” í´ë”
    OUTPUT_DIR = "filtered_shorts"  # í•„í„°ë§ëœ ì‡¼ì¸ ë¥¼ ì €ì¥í•  í´ë”
    
    # ì»¤ë®¤ë‹ˆí‹° í‚¤ì›Œë“œ ì„¤ì •
    COMMUNITY_KEYWORDS = [
        'ì»¤ë„¥ìˆœ','ì˜¤ëŠ˜','ì˜¤ëŠ˜ ì™œì¼€','ì¸í˜•ë½‘ê¸°','ê¸°ë„¤','ë°”ì§€','ë°”ì§€ ì§ˆë¬¸','ë°”ì§€ ì§ˆë¬¸ ì €ëŸ°','ë°”ì§€ ì§ˆë¬¸ ì €ëŸ° ëŒ€ë‹µ','ì²˜ìŒ','ë°˜ì§€','ë°˜ì§€ ê°€ì§€','ê¸ˆìˆœìì•„','ìš”ë‘','ìš”ë‘ ì–´ì œ','ë”œêµ','ë”œêµ ì˜','ê°™ê¸”','ì´‰ì´‰í›ˆí›ˆê°•ì•„ë””','ì§±ê°œë“¤','ì™œ ë˜ ì§€ë„','ì˜¤ëŠ˜ ì™¤ì¼€','ì˜¤ëŠ˜ ì™¤ì¼€ í˜„ì‹¤','ì§±ê¹¨','ë„ë¡¬í• ','ë„ë¡¬í•  ìˆ˜','í•˜í•„','í•˜í•„ ë¨¸í•™êµ','í•˜í•„ ë¨¸í•™êµ ì‹œí—˜ê¸°ê°„','ì˜¤ëŠ˜ ì¢€','ì˜¤ëŠ˜ ì¢€ í”¼ê³¤','ì§„ì§œ','ì§„ì§œ ì˜ì “ì ¤ë¦¬','ì¸í˜•ë½‘ê¸° ìµœì¢…','ì¸í˜•','ì¸í˜• ë½‘ê¸°','ì¸í˜• ë½‘ê¸°í• ë•Œ','ì¸í˜• ë½‘ê¸°í• ë•Œ ì–¸ë‹ˆ','ì´ëŸ° ê°€ì˜¤','ì´ëŸ° ê°€ì˜¤ ë¶€ë¦´ë•Œ','ì¸í˜•ë½‘ì„ë•Œ','ìš”ì¦˜','ìš”ì¦˜ ì†','ìš”ì¦˜ ì† í•ì¤„','ê¸€ì¼€','ê¸€ì¼€ ì¢‹ë“œ','ì´ ëˆˆë¹›','ê³µê²©','ì˜¤ëŠ˜ ë§‰íŒ','í„± ê´´ê³ ','í„± ê´´ê³  ëŒ“ê¸€','í„± ê´´ê³  ëŒ“ê¸€ ì³ë‹¤ë³¼ë•Œ','ê·¸ ì–˜ê¸°','ìš°ë¦¬ë³´ê³ ','ìš°ë¦¬ë³´ê³  ëŒ“ê¸€','í•œê±°','ì–¸ë‹ˆ','ì–¸ë‹ˆ ë¨¸ë¦¬ì†','í•œë²ˆ','ì˜¤ëŠ˜ ì†','ì´ìë²¨','ì´ìë²¨ ê¸ˆìˆœ','ì´ìë²¨ ê¸ˆìˆœê³¼ ì»¤ë„¥ìˆœ','ì—­ì‹œ','ì—­ì‹œ ë¯¸ë…€','ì œì² ','ì ¤ë¦¬','ëŒ€í•´','ë³µìŠµ','ë©”ë¡ ë¹µ','ì½”ì—½ë˜ëˆ','ìŠ¤í…Œì´í¬','ë ë‹ˆ','ëŠ¥ë ¥','ìˆìœ¼ë©´ ì ¤ë¦¬','ìˆìœ¼ë©´ ì ¤ë¦¬ ì•„ë‹Œì²™','ìˆìœ¼ë©´ ì ¤ë¦¬ ì•„ë‹Œì²™ ì§€ì›','ì†Œì°Œí‚¤','ì–¸ë‹ˆ ë´ŠëŒ','ëŒ€ì²œì‚¬','ëŒ€ì²œì‚¬ ê¸ˆìˆœ','ì´ˆë”©ë“¤','ë‘ê°œ','ì—¬ëŸ¬ëª¨ë¡œ','ì—¬ëŸ¬ëª¨ë¡œ ì†Œì‹ ','ì¤‘ë…„ì €ì”¨ë“¤','ëŠ¥ìˆ™í•œ ì—¬ì','ì§„ì§œ ì–´ë¥¸','ì–´ì œ','ì–´ì œ ëª»','ìê¸°','ì¢€ ì¶©ê²©','í‹±í†¡','í‹±í†¡ í”„ë¡¬','ì¢€ íì‡„ì ','ë°”ìœì™€ì¤‘','ì •ê¸°ì»¨í…ì¸ ','í•œë‹¤ëŠ”ê²Œ','ìƒë°©ë“¤','ìš°ë¦¬','ë³´ê³ ','ìƒê°','ë©”ëª¨','ìœ íŠœë¸Œ','ì‹ ì ¤ë¦¬','í´ë¦½','íƒ€ì„','íƒ€ì„ ì½”ë“œ','íƒ€ì„ ì½”ë“œ ë©”ëª¨','íƒ€ì„ ì½”ë“œ ë©”ëª¨í•˜ëŠ” ì–¸ëƒë“¤','íƒ€ì„ ì½”ë“œ ë©”ëª¨í•˜ëŠ” ì–¸ëƒë“¤ ì¢€','ì‹ ì…','ì‹ ì…ë“¤ì–´ì˜¤ë©´ ì˜ë‚œì²´ì¢€','ì»¨í…ì¸ ','ì»¨í…ì¸  ëë‚˜ë©´ ê²Œì‹œê¸€','ì»¨í…ì¸  ëë‚˜ë©´ ê²Œì‹œê¸€ í•˜ë‚˜','í¬ì¸íŠ¸','ì†Œìˆ˜ì','í˜¸ìº‰ìŠ¤','í˜¸ìº‰ìŠ¤ íƒ€ì„ì½”ë“œ','í˜¸ìº‰ìŠ¤ íƒ€ì„ì½”ë“œ ë©”ëª¨','ë§Œí•´','ì¡´ë‚˜','ë°©ë„','ë°©ë„ ê°œì›ƒê²¼ê¸”','ì™¤ì¼€','ì™¤ì¼€ ì´‰ì´‰','ì™¤ì¼€ ì´‰ì´‰ ì•„ë ¨','í¸ì§‘ì','ì¢†ë¬´ìœ„í‚¤','ì¢†ë¬´ìœ„í‚¤ ì •ë…','ì¢†ë¬´ìœ„í‚¤ ì •ë…ì‹œí‚¤ë©´ ë˜ê¸”','ë ˆì¦ˆëŒ€ìƒ','í•„ìš”','ìœ íŠ­','ìœ íŠ­ ë¸Œì´ë¡œê·¸','ì†ì˜·ì–˜ê¸°','ì§„ì§œ ë‹µë³€','ì§„ì§œ ë‹µë³€ ìƒê°','ëª»í–ˆê¸”','ê¸ˆì£¼ë°','í¸ì§‘í¬ì¸íŠ¸','ì¢†ëŒ','ì¢†ëŒ ì˜ˆëŠ¥','ì¢†ëŒ ì˜ˆëŠ¥ ì˜ˆê³ í¸','ëŒ•ì´','ì—í”„','ì—í”„ë”','ëˆ„ê°€','ì›ƒê¸°','ê°€ìœ„','ì ¤ì‚¬ì›ë“¤','ì ¤ì‚¬ì›ë“¤ì˜ ìƒˆë¡œìš´ íšŒì˜','ì‹œì‘','êµ°í•˜','ë°”ì§€ ë‚´ë¦¬ëŠ” ì§ˆë¬¸','ë¬´ìŠ¨ë¹µ','ì°¸ê³ ','í”„ë¡¬','í”„ë¡¬ ì„ ì˜ˆë§¤','í”„ë¡¬ ì„ ì˜ˆë§¤ ê³ ë¯¼','ì–´ì°¨í”¼','ì–´ì°¨í”¼ ìš°ë¦¬','ì–˜ê¸°','ì–˜ê¸°í•˜ëŠ” ì›ƒê¸´ ë¶€ë¶„','ì´ìœì†ì˜·','ì´ìœì†ì˜· ë§‰','ì´ìœì†ì˜· ë§‰ì…ëŠ”ì†ì˜·','ìˆìœ¼ì‹ ê°€ë³´ê¸”','ì˜ë…¼','ì˜ë…¼í•´ì£¼ëŠ” ê²ƒ','aië¡œ','aië¡œ ìˆí¼','aië¡œ ìˆí¼ ì œì‘','aië¡œ ìˆí¼ ì œì‘í•˜ëŠ” ê±°ë„','aië¡œ ìˆí¼ ì œì‘í•˜ëŠ” ê±°ë„ ìˆëˆˆë´','ì–´í•„','ì–´í•„ë˜ëŠ” ì†ì˜·','ì–´í•„ë˜ëŠ” ì†ì˜· ê¸°ì¤€','ìš°ë¦¬ ì˜ê²¬','ë³„ê±°','ë³„ê±° ì•„ë‹ ìˆ˜','ì¤‘ìš”í•œê±´ íŒ¬','ê·¸ëƒ¥','ê·¸ëƒ¥ ê±±ì •','ë°±í—ˆê·¸','ë°±í—ˆê·¸ ìœ or','ì•í—ˆê·¸ì¤‘','ìš°ë¦°','ìš°ë¦° ì‘ì›ì¡°','ìš°ë¦° ì‘ì›ì¡° ì •ë„ê¸”','ë¶€ì‹¬','ì†Œì°Œí‚¤ í˜ì• ','ì†Œì°Œí‚¤ í˜ì•  ì˜','ì–´í•„ì–˜ê¸°','íƒ€ì„ì½”ë“œ','íƒ€ì„ì½”ë“œ ë ê±°ê°™ê¸”','ë­”ê°€','ë­”ê°€ ë”','ë­”ê°€ ë” ì¢‹ì€ê²ƒ','ì‹œì–¸','ì´ë•Œ','ì´ë•Œ ìš¸ë©´','ê±±ì •í–ˆê¸”','ê¿€ë–¨ì–´ì§ˆ ë•Œ','ê¿€ë–¨ì–´ì§ˆ ë•Œ ìš°ë¦¬','ê¿€ë–¨ì–´ì§ˆ ë•Œ ìš°ë¦¬ ë¬´ìŠ¨ë§','ì‹œê¸°','12ì›”','í•œë‹¬ì´','ì‹œê°„ìˆê¸”','ì²­ì£¼ì—¬ìêµë„ì†Œ','ì£¼ì¸ë‹˜','ë‹¤ì‹œë³´ê¸°','ë‹¤ì‹œë³´ê¸° ì‹±í¬','ë°© ëŠì–´ê°€ê¸¸ ì˜','ë’·ë¶€ë¶„','ë’·ë¶€ë¶„ ì¡´ë‚˜','ì‹±í¬','ì‹±í¬ ë­','ì˜¤ëŠ˜ê±°','ì˜¤ëŠ˜ê±° ì‹±í¬','ì˜¤ëŠ˜ê±° ì‹±í¬ ì•ˆ','ì˜¤ëŠ˜ê±° ì‹±í¬ ì•ˆë§ëŠ”ê±´ ë¬¸í™”ìœ ì‚°','ì˜¤ëŠ˜ê±° ì‹±í¬ ì•ˆë§ëŠ”ê±´ ë¬¸í™”ìœ ì‚° í›¼ì†','ë§ˆì§€ë§‰ì¯¤','ë§ˆì§€ë§‰ì¯¤ ë°€ì€ ì•„ì˜ˆ','ë³µìŠµ ë­','í™”ë…¹','ì†ëª©','ì „ì™„ê·¼','ì´ë²ˆ','ì´ë²ˆ íŒ¬','ì´ë²ˆ íŒ¬ë¯¸ëŠ” í¬ì˜¹','ì˜¤ëŠ˜ê±° ë³µìŠµ','ì˜¤ëŠ˜ê±° ë³µìŠµ ëª»','í—ˆê·¸','ì™€ë½','ë°© ì‹œí—˜ê¸°ê°„','ë´Šì¹œ','ì´ëŸ° ê±´','ì´ëŸ° ê±´ ì•ˆ','ì˜¤ëŠ˜ ë­”ê°€','ì˜¤ëŠ˜ ë­”ê°€ ë¯¿ìŒì§','ê·¸ ì™€ì¤‘','ìŠ¤í°ì§€ë°¥','ì»¤ë¹„','ì»¤ë¹„ ê°€ì ¸ê°€ë©´ ì•ˆëŒ€','ëŒ• ì•½ê°„','ëŒ• ì•½ê°„ ì´‰ì´‰','ëŒ•ë ë•Œ ë†ˆ ì¢‹ê¸”','ëŒ• ì˜¤ëŠ˜','ê²‰ë¶€ì†ì´‰','ì„ëª…','ì–¸ë‹ˆ ì˜¤ëŠ˜','ì¼ì°','ì¼ì° í‘¹','ì˜¤ëŠ˜ ìœ ë…','ì˜¤ëŠ˜ ìœ ë… ë‘ë¶€','ì‚¬ì‹¤','ì‚¬ì‹¤ ë‹¤ìŒì£¼','ì—¬ëŸ¬ê°€ì§€','ë°© ë³µìŠµì¤‘','ë°°ì›…','ë°°ì›…í• ë•Œ','ë°°ì›…í• ë•Œ ì‚°íƒ€ê±¸','ë°ìˆ­ì§­ëŒ•','ë³¸ì¸','ë¬´ìŒ','ë¬´ìŒê°™ì„ ë•Œ','ê¸ˆì£¼','ê¸ˆì£¼ì˜ ë°ì´íŠ¸','ê¸ˆì£¼ì˜ ë°ì´íŠ¸ ì „ë¶€','ê¸ˆì£¼ì˜ ë°ì´íŠ¸ ì „ë¶€ ì‹¤ì‹œê°„','ë²Œì¨','ê¸ˆìˆœì–¸ë‹ˆ','ë‹¤ìŒì£¼','ë‹¤ìŒì£¼ ì¼ì •','ì¼ ì²˜ë¦¬','ì¡ëŒ•','ì˜ìƒ','ì˜ìƒ ì‹±í¬','ì˜ìƒ ì‹±í¬ ê²¨ìš°','ì™œì¼€','ë½‘ê¸°','ì§ˆë¬¸','ì €ëŸ°','ëŒ€ë‹µ','ê°€ì§€','ìˆœìì•„','ì´‰ì´‰','í›ˆí›ˆ','ì•„ë””','ì§€ë„','í˜„ì‹¤','í•™êµ','ì‹œí—˜','ê¸°ê°„','í”¼ê³¤','ì˜ì “','ìµœì¢…','ê°€ì˜¤','í•ì¤„','ì¢‹ë“œ','ëˆˆë¹›','ë§‰íŒ','ê´´ê³ ','ëŒ“ê¸€','ë¨¸ë¦¬','ê¸ˆìˆœ','ë¯¸ë…€','ì§€ì›','ë´ŠëŒ','ì†Œì‹ ','ì¤‘ë…„','ì €ì”¨ë“¤','ì—¬ì','ì–´ë¥¸','ì¶©ê²©','íì‡„ì ','ì™€ì¤‘','ì •ê¸°','ì½”ë“œ','ì–¸ëƒë“¤','ì²´ì¢€','ê²Œì‹œ','í•˜ë‚˜','ê°œì›ƒê²¼ê¸”','ì•„ë ¨','ì¢†ë¬´','ìœ„í‚¤','ì •ë…','ë˜ê¸”','ë ˆì¦ˆ','ëŒ€ìƒ','ë¸Œì´','ë¡œê·¸','ì†ì˜·','ë‹µë³€','í¸ì§‘','ì˜ˆëŠ¥','ì˜ˆê³ í¸','ì‚¬ì›ë“¤','íšŒì˜','ì˜ˆë§¤','ê³ ë¯¼','ë¶€ë¶„','ê°€ë³´','ai','ìˆí¼','ì œì‘','ê±°ë„','ìˆëˆˆë´','ê¸°ì¤€','ì˜ê²¬','ê±±ì •','or','í—ˆê·¸ì¤‘','ì‘ì›ì¡°','ì •ë„','í˜ì• ','ë ê±°ê°™ê¸”','ìš¸ë©´','í–ˆê¸”','ë¬´ìŠ¨ë§','ì‹œê°„','ìˆê¸”','ì²­ì£¼','ì—¬ìêµë„ì†Œ','ë‹¤ì‹œ','ë³´ê¸°','ë¬¸í™”ìœ ì‚°','í›¼ì†','ì•„ì˜ˆ','ì „ì™„','í¬ì˜¹','ë¯¿ìŒ','ì•ˆëŒ€','ì•½ê°„','ì¢‹ê¸”','ë¶€ì†','ìœ ë…','ë‘ë¶€','ë³µìŠµì¤‘','ì‚°íƒ€','ë°ì´íŠ¸','ì „ë¶€','ì‹¤ì‹œê°„','ìˆœì–¸ë‹ˆ','ì¼ì •','ì²˜ë¦¬','ê²¨ìš°'
    ]
    
    # í•„í„°ë§ ì„ê³„ê°’ (0~1)
    MATCH_THRESHOLD = 0.02  # 2% ì´ìƒ ë§¤ì¹­ë˜ë©´ í¬í•¨
    # =============================================
    
    # í´ë” ì¡´ì¬ í™•ì¸
    if not os.path.exists(SHORTS_FOLDER):
        print(f"âŒ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {SHORTS_FOLDER}")
        print(f"ğŸ’¡ '{SHORTS_FOLDER}' í´ë”ë¥¼ ìƒì„±í•˜ê³  ì‡¼ì¸  ì˜ìƒë“¤ì„ ë„£ì–´ì£¼ì„¸ìš”")
        return
    
    # ffmpeg í™•ì¸
    try:
        subprocess.run(['ffmpeg', '-version'], capture_output=True, check=True)
        subprocess.run(['ffprobe', '-version'], capture_output=True, check=True)
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âŒ ffmpeg/ffprobeê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤!")
        return
    
    print("ğŸ¬ ì‡¼ì¸  ë°°ì¹˜ ë¶„ì„ ì‹œìŠ¤í…œ")
    print("=" * 80)
    print(f"ğŸ“ ì…ë ¥ í´ë”: {SHORTS_FOLDER}")
    print(f"ğŸ“ ì¶œë ¥ í´ë”: {OUTPUT_DIR}")
    print(f"ğŸ¯ ë§¤ì¹­ ì„ê³„ê°’: {MATCH_THRESHOLD:.0%}")
    print(f"ğŸ”‘ ì»¤ë®¤ë‹ˆí‹° í‚¤ì›Œë“œ: {len(COMMUNITY_KEYWORDS)}ê°œ")
    print("=" * 80)
    
    # ë°°ì¹˜ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    processor = ShortsBatchProcessor(
        shorts_folder=SHORTS_FOLDER,
        output_dir=OUTPUT_DIR,
        community_keywords=COMMUNITY_KEYWORDS,
        match_threshold=MATCH_THRESHOLD
    )
    
    # 1. ëª¨ë“  ì‡¼ì¸  ë¶„ì„
    all_results = processor.process_all_shorts()
    
    if not all_results:
        print("\nâŒ ë¶„ì„í•  ìˆ˜ ìˆëŠ” ì‡¼ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤")
        return
    
    # 2. ì£¼ì œ ëª¨ë¸ë§
    all_results = processor.add_topic_modeling(all_results)
    
    # 3. ì œëª© ìƒì„±
    all_results = processor.generate_titles(all_results)
    
    # 4. í•„í„°ë§ ë° íŒŒì¼ ë³µì‚¬
    filtered_results = processor.filter_and_copy_shorts(all_results)
    
    # 5. ë¦¬í¬íŠ¸ ì €ì¥
    processor.save_reports(all_results, filtered_results)
    
    # ì™„ë£Œ!
    print("\n" + "=" * 80)
    print("ğŸ‰ ì™„ë£Œ!")
    print("=" * 80)
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {os.path.abspath(OUTPUT_DIR)}")
    print(f"ğŸ“Š ë¶„ì„ëœ ì‡¼ì¸ : {len(all_results)}ê°œ")
    print(f"âœ… í•„í„°ë§ í†µê³¼: {len(filtered_results)}ê°œ")
    print(f"ğŸ“ˆ í•„í„°ë§ ë¹„ìœ¨: {len(filtered_results)/max(len(all_results), 1):.1%}")
    
    if filtered_results:
        print(f"\nğŸ† Top 5 ì‡¼ì¸  (ë§¤ì¹­ ì ìˆ˜ìˆœ):")
        top_5 = sorted(filtered_results, key=lambda x: x['match_score'], reverse=True)[:5]
        for i, result in enumerate(top_5, 1):
            print(f"  {i}. {result.get('title', 'N/A')}")
            print(f"     ë§¤ì¹­: {result['match_score']:.0%} | ê°ì •: {result['sentiment']}")
    
    print("\nğŸ’¡ ìƒì„±ëœ íŒŒì¼:")
    print(f"  - {len(filtered_results)}ê°œì˜ í•„í„°ë§ëœ ì‡¼ì¸  ì˜ìƒ")
    print(f"  - analysis_report.json (ìƒì„¸ ë¶„ì„ ê²°ê³¼)")
    print(f"  - report.txt (ì½ê¸° ì‰¬ìš´ ë¦¬í¬íŠ¸)")
    print(f"  - summary.csv (ì—‘ì…€ìš© ìš”ì•½)")
    print("=" * 80)


if __name__ == "__main__":
    try:
        from faster_whisper import WhisperModel
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.decomposition import LatentDirichletAllocation
    except ImportError:
        print("âŒ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
        print("\npip install faster-whisper scikit-learn")
        print("pip install kiwipiepy  # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° (ê¶Œì¥)")
        exit(1)
    
    main()